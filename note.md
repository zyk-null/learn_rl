# 强化学习经典算法实现

## value-base价值学习

### TD（时序差分）算法

蒙特卡洛方法对价值函数的增量更新方式（$\alpha$表示更新步长） ：
$$
U_t \gets U_t + \alpha [G_t - U_t]
$$

时序差分算法用当前获得的奖励，加上下一个状态的价值估计来作为在当前状态会获得的回报，不需要等到结束就能估计出结果（$\gamma$为折现率）。
$$
\begin{align}
& U_t = R_t + \gamma \cdot U_{t + 1} \\
& y_t = R_t + \gamma \cdot U_{t + 1} \text{更接近真实值} \\
& y_t - R_t = R_t + \gamma \cdot U_{t + 1} - U_t \text{, 称作TD error}
\end{align}
$$
时序差分算法将TD error与步长的乘积作为状态价值的更新量，即：
$$
U_t \gets U_t + \alpha [R_t + \gamma \cdot U_{t + 1} - U_t]
$$

### `Sarsa`算法

可以直接用时序差分算法来估计动作价值函数：
$$
Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) -Q(s_t, a_t)]
$$
然后用贪婪算法来选取在某个状态下动作价值最大的那个动作，即$arg max_a Q(s, a)$。

这样似乎已经形成了一个完整的强化学习算法：用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。

然而这个简单的算法存在两个需要进一步考虑的问题。

* 如果要用时序差分算法来准确地估计策略的状态价值函数，需要用极大量的样本来进行更新。但实际上可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。可以这么做的原因是策略提升可以在策略评估未完全进行的情况进。
* 如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。简单常用的解决方案是不再一味使用贪婪算法，而是采用一个$\epsilon$-greedy策略：有的$\epsilon$概率采用动作价值最大的那个动作，另外有$1 - \epsilon$的概率从动作空间中随机采取一个动作。

``Sarsa`` 的具体算法如下：

> $初始化 Q(s, a)$
>
> $for \ 序列 e = 1 \to e \ do：$
>
> ​        $得到初始状态s$
>
> ​        $用 \epsilon-greedy 策略根据选择当前状态s下的动作a$
>
> ​        $ for\ 时间步t = 1 \to T \  do :$
>
> ​                $ 得到环境反馈的r, s'$
>
> ​                $用\epsilon-greedy 策略根据选择当前状态s'下的动作a'$
>
> ​                $Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) -Q(s_t, a_t)]$
>
> ​                 $s \gets s', a \gets a'$



### `Q-learning`算法

除了 `Sarsa`，还有一种非常著名的基于时序差分算法的强化学习算法——`Q-learning`。`Q-learning` 和 `Sarsa` 的最大区别在于 `Q-learning` 的时序差分更新方式为：

<<<<<<< HEAD
![image-20240909212259058](D:\User\Desktop\current\learn_rl\image\note\image-20240909212259058.png)

`Q-learning` 算法的具体流程如下：
=======
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$
Q-learning 算法的具体流程如下：
>>>>>>> b5c083af3f0ff93c42ed3284b77583cf38529ced

![image-20240909212340385](.\image\note\image-20240909212340385.png)

<<<<<<< HEAD
`Q-learning`是在估计$Q^*$，而而 `Sarsa` 估计当前$\epsilon$-greedy策略的动作价值函数。需要强调的是，`Q-learning` 的更新**并非必须使用当前贪心策略**$argmax_aQ(s,a)$采样得到的数据，因为给定任意$(s,a,r,s')$都可以直接根据更新公式来更新$Q$，为了探索，通常使用一个$\epsilon$-greedy策略来与环境交互。`Sarsa` 必须使用当前$\epsilon$-greedy策略采样得到的数据，因为它的更新中用到的$Q(s',a')$中的$a'$是**当前策略**在$s'$下的动作。我们称 `Sarsa` 是**在线策略**（on-policy）算法，称 `Q-learning` 是**离线策略**（off-policy）算法，这两个概念强化学习中非常重要。
=======
q-learning是在估计$Q^*$，而而 Sarsa 估计当前$\epsilon$-greedy策略的动作价值函数。

需要强调的是，Q-learning 的更新并非必须使用当前贪心策略$argmax_aQ(s,a)$采样得到的数据，因为给定**任意**$(s,a,r,s')$都可以直接根据更新公式来更新$Q$，为了探索，通常使用一个$\epsilon$-greedy策略来与环境交互。

Sarsa 必须使用当前$\epsilon$-greedy策略采样得到的数据，因为它的更新中用到的$Q(s',a')$中的$a'$是当前策略在$s'$下的动作。我们称 Sarsa 是**在线策略**（on-policy）算法，称 Q-learning 是**离线策略**（off-policy）算法，这两个概念强化学习中非常重要。
>>>>>>> b5c083af3f0ff93c42ed3284b77583cf38529ced

> 称采样数据（收集经验）的策略为**行为策略**（behavior policy），称使用这些数据来更新的策略（更新表格）为**目标策略**（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不一定是同一个策略。`Sarsa` 是典型的在线策略算法，而 `Q-learning` 是典型的离线策略算法。判断二者类别的一个重要手段是看**计算TD target的数据是否来自当前的策略**，如图 所示。具体而言：
>
<<<<<<< HEAD
> - 对于 `Sarsa`，它的更新公式必须使用来自当前策略采样得到的五元组$(s,a,r,s',a')$，因此它是在线策略学习方法；
> - 对于 `Q-learning`，它的更新公式使用的是四元组$(s,a,r,a')$来更新当前状态动作对$Q(s,a)$的价值，数据中的$s$和$a$是给定的条件，$r$和$s'$皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。
=======
> - 对于 Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组$(s,a,r,s',a')$，因此它是在线策略学习方法；
> - 对于 Q-learning，它的更新公式使用的是四元组$(s,a,r,s')$（把$a'$消掉了）来更新当前状态动作对$Q(s,a)$的价值，数据中的$s$和$a$是给定的条件，$r$和$s'$皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。
>>>>>>> b5c083af3f0ff93c42ed3284b77583cf38529ced
>
> ![img](.\image\note\400.78f393db.png)
>
> 离线策略算法能够重复使用过往训练样本，往往具有更小的样本复杂度，也因此更受欢迎。
>
> 离线策略可以通过经验回放来进行更新，同策略不行，因为收集经验时的行为策略不同于想要训练出来的目标策略。

<<<<<<< HEAD
![image-20241027152025965](D:\User\Desktop\current\learn_rl\image\note\image-20241027152025965.png)
=======
> sarsa流程：
>
> 

### 比较

Q-learning 和 SARSA 都是强化学习中的两种常用算法，主要区别在于它们的更新策略。以下是两者的比较：

#### 1. 更新方式（策略）不同：

- **Q-learning**：是一种*off-policy*算法。它不使用当前的策略来更新Q值，而是使用目标策略来更新，即**假设在当前状态下采取最优动作（greedy action）**。因此，它更新的是关于最优策略的Q值。

  **公式**：
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
  $$
  在这里，$\max_{a'} Q(s', a')$ 是在下一状态 $s'$ 中选择最优动作 $a'$ 的Q值。

- **SARSA**：是一种*on-policy*算法。它使用当前策略来更新Q值，即按照当前策略选择的动作来更新。因此，SARSA在更新Q值时，是基于**当前实际采取的动作序列**，而不是假设最优策略。

  **公式**：
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$
  在这里，$Q(s', a')$ 是根据当前策略选择的动作 $a'$ 在下一状态 $s'$ 的Q值。
  
  > Sarsa在 $ t $ 时刻**表格更新前**选取 $ t+1 $ 时刻的动作，Q-Learning则在 $ t $ 时刻**表格更新后**选取 $ t+1 $ 时刻的动作。

#### 2. **探索和利用的平衡：**

- **Q-learning** 假设代理始终采取最优动作，因此在探索与利用之间更加倾向于“利用”（利用现有的知识去做决策）。这种方式可能在训练过程中会导致Q-learning对噪声比较敏感，因为它直接尝试选择最大化回报的动作。

- **SARSA** 则更注重实际执行的动作，能够更好地平衡探索与利用。由于它在更新Q值时考虑了探索策略，SARSA在某些场景下可以更好地应对高噪声的环境。

#### 3. **收敛速度和稳定性：**

- **Q-learning**：在理论上收敛于最优Q值，但由于它不依赖当前的策略，所以可能会经历更多的不稳定性和波动。如果环境中包含大量随机性或噪声，Q-learning 的更新可能更慢。

- **SARSA**：因为它基于当前的策略，所以训练过程中相对稳定，但收敛速度可能会慢一些。它更适用于噪声较大的环境，因为它遵循的是代理当前的动作选择逻辑。

sarsa更新滞后，会更保守一些，ql更激进一些

>>>>>>> b5c083af3f0ff93c42ed3284b77583cf38529ced

### `DQN`

使用函数来拟合Q表

![img](D:\User\Desktop\current\learn_rl\image\note\640.46b13e89.png)

损失函数：使用TD Error

![image-20241027153737597](D:\User\Desktop\current\learn_rl\image\note\image-20241027153737597.png)

#### 经验回放

维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用。

（1）使样本满足独立假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。

（2）提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

对于“意外情况“，可以分配较高的权重。

#### 高估问题

Q学习算法有一个缺陷：用Q学习训练出的DQN会高估真实的价值，而且高估通常是非均匀的。这个缺陷导致DQN的表现很差。高估问题并不是DQN模型的缺陷，而是Q学习算法的缺陷。Q学习产生高估的原因有两个：

* 第一，自举导致偏差的传播；
* 第二，最大化导致TD目标高估真实价值。

为了缓解高估，需要从导致高估的两个原因下 手，改进Q学习算法。双Q学习算法是一种有效的改进，可以大幅缓解高估及其危害。

#### DDQN

![image-20241027185441076](D:\User\Desktop\current\learn_rl\image\note\image-20241027185441076.png)

#### 对决网络(DuelingNetwork)

![image-20241027203058919](D:\User\Desktop\current\learn_rl\image\note\image-20241027203058919.png)

![image-20241027203108022](D:\User\Desktop\current\learn_rl\image\note\image-20241027203108022.png)

> $\max D_*(s, a) = 30$

Q 网络被建模为：最优优势函数$D_*$ + 最优状态价值函数$V_*$

$V(s)$为状态价值函数，$D(s)$而则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性。训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到值。

![img](D:\User\Desktop\current\learn_rl\image\note\640.455bc383.png)

将状态价值函数和优势函数分别建模的好处在于：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。在下图所示的驾驶车辆游戏中，智能体注意力集中的部位被显示为橙色，当智能体前面没有车时，车辆自身动作并没有太大差异，此时智能体更关注状态价值，而当智能体前面有车时（智能体需要超车），智能体开始关注不同动作优势值的差异。

![img](D:\User\Desktop\current\learn_rl\image\note\641.b74c8db2.png)

对于 Dueling DQN 中的公式，它存在对于值和值建模不唯一性的问题。例如，对于同样的值，如果将值加上任意大小的常数，再将所有值减去，则得到的值依然不变，这就导致了训练的不稳定性。为了解决这一问题，Dueling DQN加上了一个项$max D(s, a)$。此时，可以确保值建模的唯一性。在实现过程中，我们还可以用平均代替最大化操作。

“为什么 Dueling DQN 会比 DQN 好？”部分原因在于 Dueling DQN 能更高效学习状态价值函数。每一次更新时，函数都会被更新，这也会影响到其他动作的值。而传统的 DQN 只会更新某个动作的值，其他动作的值就不会更新。因此，Dueling DQN 能够更加频繁、准确地学习状态价值函数。

## policy-base策略学习
